import argparse
import os
import pathlib
import re
import time
import datetime
import random
import pandas as pd
import torch
from torch.utils.data import DataLoader
from analyze_predictions import *
from dataset import build_poisoned_training_set, build_testset
from deeplearning import evaluate_badnets, optimizer_picker, train_one_epoch, predict_single_image
from models import BadNet
import glob
import os
import json
from torchvision.utils import save_image

parser = argparse.ArgumentParser(description='Reproduce the basic backdoor attack in "Badnets: Identifying vulnerabilities in the machine learning model supply chain".')
parser.add_argument('--dataset', default='ImageNet', help='Which dataset to use (MNIST or CIFAR10 or ImageNet, default: MNIST)')
parser.add_argument('--nb_classes', default=10, type=int, help='number of the classification types')
parser.add_argument('--load_local', action='store_true', help='train model or directly load model (default true, if you add this param, then load trained local model to evaluate the performance)')
parser.add_argument('--loss', default='mse', help='Which loss function to use (mse or cross, default: mse)')
parser.add_argument('--optimizer', default='sgd', help='Which optimizer to use (sgd or adam, default: sgd)')
parser.add_argument('--epochs', default=100, help='Number of epochs to train backdoor model, default: 100')
parser.add_argument('--batch_size', type=int, default=64, help='Batch size to split dataset, default: 64')
parser.add_argument('--num_workers', type=int, default=0, help='Batch size to split dataset, default: 64')
parser.add_argument('--lr', type=float, default=0.001, help='Learning rate of the model, default: 0.001')
parser.add_argument('--download', action='store_true', help='Do you want to download data ( default false, if you add this param, then download)')
parser.add_argument('--data_path', default='./data/', help='Which dataset to use (badnets-mnist/dataset/imagenet100/ or ./dataset/ Place to load dataset (default: ./dataset/)')
parser.add_argument('--device', default='cpu', help='device to use for training / testing (cpu, or cuda:1, default: cpu)')
# poison settings
parser.add_argument('--poisoning_rate', type=float, default=0.1, help='poisoning portion (float, range from 0 to 1, default: 0.1)')
parser.add_argument('--trigger_label', type=int, default=1, help='The NO. of trigger label (int, range from 0 to 10, default: 0)')
parser.add_argument('--trigger_path', default="/root/badnets-mnist/triggers/trigger_white.png", help='Trigger Path (default: ./triggers/trigger_white.png)')
parser.add_argument('--trigger_size', type=int, default=5, help='Trigger Size (int, default: 5)')

args = parser.parse_args()

def main():
    print("{}".format(args).replace(', ', ',\n'))

    if re.match('cuda:\d', args.device):
        cuda_num = args.device.split(':')[1]
        os.environ['CUDA_VISIBLE_DEVICES'] = cuda_num
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # if you're using MBP M1, you can also use "mps"

    # create related path
    pathlib.Path("./checkpoints/").mkdir(parents=True, exist_ok=True)
    pathlib.Path("./logs/").mkdir(parents=True, exist_ok=True)

    print("\n# load dataset: %s " % args.dataset)
    dataset_train, args.nb_classes = build_poisoned_training_set(is_train=True, args=args)
    dataset_val_clean, dataset_val_poisoned = build_testset(is_train=False, args=args)
    
    data_loader_train        = DataLoader(dataset_train,         batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)
    data_loader_val_clean    = DataLoader(dataset_val_clean,     batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)
    data_loader_val_poisoned = DataLoader(dataset_val_poisoned,  batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers) # shuffle éšæœºåŒ–

    # 1ã€ä¿å­˜æ”»å‡»åçš„å›¾ç‰‡
    print("Saving clean images...")
    save_images(data_loader_val_clean, "/root/autodl-tmp/ImageNet/attack_after", prefix="clean")
    print("Saving poisoned images...")
    save_images(data_loader_val_poisoned, "/root/autodl-tmp/ImageNet/attack_after", prefix="poisoned")
    print("Saving 100 clean images per class...")
            # MNISTæ•°æ®é›†çš„åˆ†ç±»
    class_names = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]
    save_sampled_images(data_loader_val_clean, "/root/autodl-tmp/ImageNet/attack_after", class_names, num_samples_per_class=100, prefix="clean")
    print("Saving 100 poisoned images per class...")
    save_sampled_images(data_loader_val_poisoned, "/root/autodl-tmp/ImageNet/attack_after", class_names, num_samples_per_class=100, prefix="poisoned")
  
    model = BadNet(input_channels=dataset_train.channels, output_num=args.nb_classes).to(device)
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = optimizer_picker(args.optimizer, model.parameters(), lr=args.lr)

    basic_model_path = "./checkpoints/badnet-%s.pth" % args.dataset
    start_time = time.time()
    if os.path.exists(basic_model_path):  
        print("âœ… å‘ç°å·²è®­ç»ƒçš„æ¨¡å‹ï¼ŒåŠ è½½ä¸­...")
        model.load_state_dict(torch.load(basic_model_path), strict=True)
        test_stats = evaluate_badnets(data_loader_val_clean, data_loader_val_poisoned, model, device)
        if test_stats['clean_acc'] < 0.85:  
            print("âš ï¸ è™½ç„¶åŠ è½½äº†æ¨¡å‹ï¼Œä½†å‡†ç¡®ç‡è¿‡ä½ï¼Œå¯èƒ½ä»éœ€è®­ç»ƒï¼")
            args.load_local = False
        else:
            print(f"âœ… å‘ç°å·²è®­ç»ƒæ¨¡å‹ï¼Œå½“å‰å‡†ç¡®ç‡: {test_stats['clean_acc']:.2f}")
            args.load_local = True
    else:
        print("âš ï¸ æœªå‘ç°å·²è®­ç»ƒæ¨¡å‹ï¼Œå¼€å§‹æ–°è®­ç»ƒ...")
        args.load_local = False

    if args.load_local:
        print("## Load model from : %s" % basic_model_path)
        model.load_state_dict(torch.load(basic_model_path), strict=True)
        device = "cuda"  # æˆ– "cpu"

        # MNISTæ•°æ®é›†çš„åˆ†ç±»
        class_names = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]

        # # 2ã€è¿‡æ»¤æ‰æœªè¢«æ”»å‡»æˆåŠŸçš„ä¸­æ¯’å›¾åƒ
        # folder_path = "/root/autodl-tmp/badnets-MNIST/attack_after"  # æ›¿æ¢æˆä½ çš„å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
        # filter_images_by_prediction(folder_path, model, device, class_names)
        

        # 4ã€è®¾å®šå‚æ•°
        folder_path = "/root/autodl-tmp/badnets-MNIST/ronghe"  # æ›¿æ¢ä¸ºä½ çš„å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
        output_file = "/root/badnets-mnist/prediction_results.json"  # é¢„æµ‹ç»“æœä¿å­˜è·¯å¾„
        predict_images_in_folder(folder_path, model, device, class_names, output_file)
        # ä½¿ç”¨ç¤ºä¾‹
        json_file = "/root/badnets-mnist/prediction_results.json"  # æ›¿æ¢ä¸ºä½ çš„ JSON æ–‡ä»¶è·¯å¾„
        results = analyze_predictions(json_file)
        print(results)
        folder = "/root/autodl-tmp/badnets-MNIST/attack_after"
        results, acc = predict_clean_images_and_calc_acc(folder, model, device, class_names)
    else:
        print(f"Start training for {args.epochs} epochs")
        stats = []
        for epoch in range(args.epochs):
            train_stats = train_one_epoch(data_loader_train, model, criterion, optimizer, args.loss, device)
            test_stats = evaluate_badnets(data_loader_val_clean, data_loader_val_poisoned, model, device)
            print(f"# EPOCH {epoch}   loss: {train_stats['loss']:.4f} Test Acc: {test_stats['clean_acc']:.4f}, ASR: {test_stats['asr']:.4f}\n")
            
            # save model 
            torch.save(model.state_dict(), basic_model_path)

            log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},
                            **{f'test_{k}': v for k, v in test_stats.items()},
                            'epoch': epoch,
            }

            # save training stats
            stats.append(log_stats)
            df = pd.DataFrame(stats)
            df.to_csv("./logs/%s_trigger%d.csv" % (args.dataset, args.trigger_label), index=False, encoding='utf-8')

    total_time = time.time() - start_time
    total_time_str = str(datetime.timedelta(seconds=int(total_time)))
    print('Training time {}'.format(total_time_str))

def coco_collate_fn(batch):
    """
    è‡ªå®šä¹‰ collate_fn ç”¨äºå¤„ç† COCO æ•°æ®é›†çš„æ‰¹æ¬¡æ•°æ®ã€‚
    """
    images = []
    targets = []

    for img, target in batch:
        images.append(img)
        targets.append(target)

    # å°†å›¾åƒå †å æˆä¸€ä¸ªæ‰¹æ¬¡
    images = torch.stack(images, dim=0)

    return images, targets

def predict_images_in_folder(folder_path, model, device, class_names, output_file):
    image_paths = glob.glob(os.path.join(folder_path, "*.png"))  # è·å–æ‰€æœ‰ PNG å›¾ç‰‡è·¯å¾„
    results = []
    
    for image_path in image_paths:
        result = predict_single_image(image_path, model, device, print_perform=True, class_names=class_names)
        result_info = {
            "image": os.path.basename(image_path),
            "prediction": result["index"],
            "class_name": class_names[result["index"]]
        }
        results.append(result_info)
        print(f"predict result: {result_info}")
    
    # ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶
    with open(output_file, "w") as f:
        json.dump(results, f, indent=4)
    
    print(f"Results saved to {output_file}")


def filter_images_by_prediction(folder_path, model, device, class_names):
    """éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾ç‰‡ï¼Œåˆ é™¤ prediction ä¸æ˜¯ 1 çš„å›¾ç‰‡"""
    
    image_paths = glob.glob(os.path.join(folder_path, "*.png"))  # è·å–æ‰€æœ‰ PNG å›¾ç‰‡è·¯å¾„
    keep_count = 0  # ç»Ÿè®¡ä¿ç•™çš„å›¾ç‰‡æ•°é‡
    delete_count = 0  # ç»Ÿè®¡åˆ é™¤çš„å›¾ç‰‡æ•°é‡
    
    for image_path in image_paths:
        # è¿›è¡Œæ¨¡å‹é¢„æµ‹
        result = predict_single_image(image_path, model, device, print_perform=False, class_names=class_names)
        prediction = result["index"]

  # è·å–æ–‡ä»¶å
        file_name = os.path.basename(image_path)
        # å¦‚æœ prediction ä¸æ˜¯ 1ï¼Œåˆ™åˆ é™¤è¯¥å›¾ç‰‡
        if prediction != 1 and "poisoned" in file_name:
            os.remove(image_path)
            delete_count += 1
            print(f"ğŸ—‘ï¸ å·²åˆ é™¤å›¾ç‰‡: {os.path.basename(image_path)} (prediction={prediction})")
        else:
            keep_count += 1  # ä¿ç•™çš„å›¾ç‰‡æ•°é‡
    
    print(f"âœ… å¤„ç†å®Œæˆï¼šä¿ç•™ {keep_count} å¼ å›¾ç‰‡ï¼Œåˆ é™¤ {delete_count} å¼ å›¾ç‰‡")


def save_sampled_images(data_loader, folder_path, class_names, num_samples_per_class=100, prefix="image"):
    # ç¡®ä¿ç›®æ ‡æ–‡ä»¶å¤¹å­˜åœ¨
    os.makedirs(folder_path, exist_ok=True)

    # ç»„ç»‡æ•°æ®ï¼ŒæŒ‰ç±»åˆ«å­˜å‚¨å›¾åƒ
    class_dict = {cls: [] for cls in class_names}

    # éå†æ•°æ®é›†ï¼Œå°†æ¯å¼ å›¾ç‰‡æŒ‰ç±»åˆ«å­˜å…¥å­—å…¸
    for images, labels in data_loader:
        for img, label in zip(images, labels):
            class_name = class_names[label.item()]
            class_dict[class_name].append(img)

    # ç»Ÿä¸€å­˜æ”¾å›¾ç‰‡
    image_count = 0

    # ä¸ºæ¯ä¸ªç±»åˆ«éšæœºé€‰å– num_samples_per_class å¼ å›¾ç‰‡
    for class_name, images in class_dict.items():
        sampled_images = random.sample(images, min(num_samples_per_class, len(images)))  # é˜²æ­¢æ•°æ®ä¸è¶³

        # ä¿å­˜å›¾åƒï¼Œæ‰€æœ‰å›¾åƒæ”¾åœ¨åŒä¸€æ–‡ä»¶å¤¹
        for img in sampled_images:
            img_filename = os.path.join(folder_path, f"{prefix}_{class_name}_{image_count:05d}.png")
            save_image(img, img_filename)
            print(f"âœ… Saved: {img_filename}")
            image_count += 1  # è®¡æ•°é€’å¢ï¼Œç¡®ä¿æ–‡ä»¶åå”¯ä¸€

    print("ğŸ‰ All images saved successfully!")



def  data_loader_val_clean_ronghe (folder, keyword="clean"):
    """
    ä»æ–‡ä»¶å¤¹ä¸­åŠ è½½åŒ…å«ç‰¹å®šå…³é”®å­—çš„å›¾ç‰‡æ–‡ä»¶åï¼Œå¹¶åˆ›å»ºå¯¹åº”çš„ DataLoaderã€‚

    :param folder: å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
    :param keyword: éœ€è¦åŒ¹é…çš„å…³é”®å­—
    :return: åŒ…å«åŒ¹é…å…³é”®å­—çš„å›¾åƒè·¯å¾„åˆ—è¡¨
    """
    image_paths = []
    for filename in os.listdir(folder):
        if keyword in filename.lower():  # åªé€‰å–åŒ…å« 'clean' çš„å›¾åƒæ–‡ä»¶å
            image_paths.append(os.path.join(folder, filename))
    return image_paths

def  data_loader_val_poisoned_ronghe (folder, keyword="poisoned"):
    """
    ä»æ–‡ä»¶å¤¹ä¸­åŠ è½½åŒ…å«ç‰¹å®šå…³é”®å­—çš„å›¾ç‰‡æ–‡ä»¶åï¼Œå¹¶åˆ›å»ºå¯¹åº”çš„ DataLoaderã€‚

    :param folder: å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
    :param keyword: éœ€è¦åŒ¹é…çš„å…³é”®å­—
    :return: åŒ…å«åŒ¹é…å…³é”®å­—çš„å›¾åƒè·¯å¾„åˆ—è¡¨
    """
    image_paths = []
    for filename in os.listdir(folder):
        if keyword in filename.lower():  # åªé€‰å–åŒ…å« 'clean' çš„å›¾åƒæ–‡ä»¶å
            image_paths.append(os.path.join(folder, filename))
    return image_paths

# ä¿å­˜å…¨éƒ¨å›¾åƒçš„å‡½æ•°
def save_images(data_loader, folder_path, prefix="image"):
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)  # å¦‚æœæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ

    image_count = 0
    for i, (images, labels) in enumerate(data_loader):
        for j in range(images.size(0)):
            img = images[j]  # è·å–å½“å‰å›¾ç‰‡
            label = labels[j]  # è·å–å½“å‰æ ‡ç­¾

            # ä¿å­˜å›¾åƒ
            img_filename = os.path.join(folder_path, f"{prefix}_{image_count}.png")
            save_image(img, img_filename)  # ä¿å­˜å›¾åƒ

            # ä¿å­˜æ ‡ç­¾
            label_filename = os.path.join(folder_path, f"{prefix}_{image_count}_label.txt")
            with open(label_filename, 'w') as label_file:
                # å‡è®¾æ ‡ç­¾æ˜¯ä¸€ä¸ªå•ä¸€çš„ç±»ç´¢å¼•ï¼Œä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ä¸ºå¤šç±»æ ‡ç­¾ç­‰æ ¼å¼
                label_file.write(str(label.item()))  # ä¿å­˜æ ‡ç­¾ï¼ˆè½¬æ¢ä¸ºæ•°å­—ï¼‰

            image_count += 1
            print(f"Saved {img_filename} and {label_filename}")

def predict_clean_images_and_calc_acc(folder_path, model, device, class_names):
    image_paths = glob.glob(os.path.join(folder_path, "*.png"))  # è·å–æ‰€æœ‰ PNG å›¾ç‰‡è·¯å¾„
    results = []
    total = 0
    correct = 0

    for image_path in image_paths:
        filename = os.path.basename(image_path)
        if 'clean' not in filename:
            continue  # è·³è¿‡ä¸å« "clean" çš„å›¾ç‰‡

        result = predict_single_image(image_path, model, device, print_perform=False, class_names=class_names)
        predicted_index = result["index"]
        predicted_class_name = class_names[predicted_index]

        total += 1
        if predicted_class_name in filename:
            correct += 1

        result_info = {
            "image": filename,
            "prediction": predicted_index,
            "predicted_class_name": predicted_class_name,
            "correct": predicted_class_name in filename
        }
        results.append(result_info)
        # print(f"Predict result: {result_info}")

    acc = correct / total if total > 0 else 0
    print(f"\nTotal clean images: {total}")
    print(f"Correct predictions: {correct}")
    print(f"Accuracy: {acc:.4f}")

    return results, acc

if __name__ == "__main__":
    main()
